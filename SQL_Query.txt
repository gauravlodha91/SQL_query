# SQL_query
Enlisted the basic and get-started query for the SQL interview



------------------------------------------------------------------------------------------------------------------------------------------------------------------


Q . Find the 3rd Largest Salary from the Employee table.

**To create a Dataframe or TempTable, follow tbl_creation.txt and search `Emp_tbl_for_Largest_Salary`.

+---------------+--------------------+------+-------------+
|       Emp_name|                Role|Salary|Date of Birth|
+---------------+--------------------+------+-------------+
|       John Doe|   Software Engineer| 80000|   1985-05-15|
|     Jane Smith|     Product Manager| 90000|   1982-11-20|
|Michael Johnson|        Data Analyst| 65000|   1988-03-01|
|    Emily Davis|         UX Designer| 75000|   1990-08-12|
|      David Lee|          IT Support| 50000|   1992-01-30|
|      Sarah Kim|Marketing Coordin...| 55000|   1987-06-25|
|     Tom Wilson|     Project Manager| 85000|   1983-09-18|
|  Jessica Brown|       HR Specialist| 60000|   1991-04-03|
|  Robert Garcia|Sales Representative| 70000|   1986-11-28|
|  Olivia Nguyen|  Accounting Analyst| 75000|   1989-07-10|
+---------------+--------------------+------+-------------+

Created Temp table as `EmpInfo` and spark Dataframe as `df`

SQL: "SELECT * FROM EmpInfo ORDER BY Salary DESC LIMIT 2,1"

Spark.SQL  : spark.sql("SELECT * FROM EmpInfo ORDER BY Salary DESC LIMIT 10 OFFSET 1").show()

Pyspark : result_df = spark.createDataFrame(df.orderBy("salary", ascending=False).limit(3).tail(1))
          result_df.show(truncate=False)


 
Advanced Version Of Largest Salary : 


Q. Find the `N` highest salary from the table without using Top/Limit keyword

**To create a Dataframe or TempTable, follow tbl_creation.txt and search `Emp_tbl_for_Largest_Salary`.

+---------------+--------------------+------+-------------+
|       Emp_name|                Role|Salary|Date of Birth|
+---------------+--------------------+------+-------------+
|       John Doe|   Software Engineer| 80000|   1985-05-15|
|     Jane Smith|     Product Manager| 90000|   1982-11-20|
|Michael Johnson|        Data Analyst| 65000|   1988-03-01|
|    Emily Davis|         UX Designer| 75000|   1990-08-12|
|      David Lee|          IT Support| 50000|   1992-01-30|
|      Sarah Kim|Marketing Coordin...| 55000|   1987-06-25|
|     Tom Wilson|     Project Manager| 85000|   1983-09-18|
|  Jessica Brown|       HR Specialist| 60000|   1991-04-03|
|  Robert Garcia|Sales Representative| 70000|   1986-11-28|
|  Olivia Nguyen|  Accounting Analyst| 75000|   1989-07-10|
+---------------+--------------------+------+-------------+

SQL : select * from EmpInfo e1 where `< N - 1 >` = (select count(distinct(salary)) from EmpInfo e2 where e2.salary > e1.salary

Spark.SQL: spark.sql("select * from EmpInfo e1 where `< N - 1 >` = (select count(distinct(salary)) from EmpInfo e2 where e2.salary > e1.salary) ").show()

Pyspark : 

A.
                    from pyspark.sql import Window
                    from pyspark.sql.functions import count, col, desc, dense_rank
                    
                    # Create a window specification
                    window_spec = Window.orderBy(desc("Salary"))
                    
                    # Apply the logic
                    result = (spark.table("EmpInfo")
                        .withColumn("salary_rank", dense_rank().over(window_spec))
                        .filter(col("salary_rank") == `< N >`)
                        .drop("salary_rank"))
                    
                    # Show the result
                    result.show(truncate=False)


B.
                    
                    from pyspark.sql.functions import count, col
                    
                    # Main query using join
                    result = (spark.table("EmpInfo").alias("e1")
                        .join(spark.table("EmpInfo").alias("e2"), col("e2.Salary") > col("e1.Salary"), "left")
                        .groupBy("e1.Emp_name", "e1.Role", "e1.Salary", "e1.Date of Birth")
                        .agg(count("e2.Salary").alias("higher_salary_count"))
                        .filter(col("higher_salary_count") == `< N - 1 >`)
                        .select("e1.*"))
                    
                    # Show the result
                    result.show(truncate=False)


--------------------------------------------------------------------------------------------------------------------------------------------------------


Q . Delete the duplicate record from the table.


 **To create Table, follow tbl_creation.txt and search for `Emp_tbl_for_Duplicate_Records`.


+------+---------------+---------------------+------+----------+
|Emp_id|Emp_name       |Role                 |Salary|DOB       |
+------+---------------+---------------------+------+----------+
|1002  |Jane Smith     |Product Manager      |90000 |1982-11-20|
|1001  |John Doe       |Software Engineer    |80000 |1985-05-15|
|1035  |Jane Smith     |Product Manager      |90000 |1982-11-20|
|1024  |Jane Smith     |Product Manager      |90000 |1982-11-20|
|1026  |Michael Johnson|Data Analyst         |65000 |1988-03-01|
|1034  |Emily Davis    |UX Designer          |75000 |1990-08-12|
|1042  |David Lee      |IT Support           |50000 |1992-01-30|
|1050  |Sarah Kim      |Marketing Coordinator|55000 |1987-06-25|
|4052  |Tom Wilson     |Project Manager      |85000 |1983-09-18|
|1245  |Tom Wilson     |Project Manager      |85000 |1983-09-18|
|1089  |Tom Wilson     |Project Manager      |85000 |1983-09-18|
|4025  |Jessica Brown  |HR Specialist        |60000 |1991-04-03|
|1212  |Robert Garcia  |Sales Representative |70000 |1986-11-28|
|1313  |Olivia Nguyen  |Accounting Analyst   |75000 |1989-07-10|
+------+---------------+---------------------+------+----------+


SQL --  Delete from EmpInfo where Emp_id not in ( select min(Emp_id) from EmpInfo group by Emp_name, Role, Salary, DOB

Spark.SQL --  spark.sql("Delete from EmpInfo where Emp_id not in ( select min(Emp_id) from EmpInfo group by Emp_name, Role, Salary, DOB)")

Pyspark  -- 

    A.                res = spark.sql("select * from EmpInfo")
                    res.dropDuplicates(["Emp_name", "Role", "Salary", "DOB"]).show()


    B.

                    
                    from pyspark.sql.functions import row_number
                    from pyspark.sql.window import Window
                    
                    # Read the EmpInfo table
                    df = spark.table("EmpInfo")
                    
                    # Define a window partitioned by the columns that define a duplicate,
                    # and ordered by Emp_id in ascending order
                    window = Window.partitionBy("Emp_name", "Role", "Salary", "DOB").orderBy("Emp_id")
                    
                    # Add a row number within each partition
                    df_with_row_number = df.withColumn("row_num", row_number().over(window))
                    
                    # Keep only the first row (lowest Emp_id) within each partition
                    df_deduplicated = df_with_row_number.filter(df_with_row_number.row_num == 1).drop("row_num")
                    
                    # Overwrite the original table with the deduplicated data
                    df_deduplicated.write.mode("overwrite").saveAsTable("EmpInfo")
                    
                    # Show the result
                    spark.table("EmpInfo").show(truncate = False)



o/p  -->

+------+---------------+---------------------+------+----------+
|Emp_id|Emp_name       |Role                 |Salary|DOB       |
+------+---------------+---------------------+------+----------+
|1042  |David Lee      |IT Support           |50000 |1992-01-30|
|1034  |Emily Davis    |UX Designer          |75000 |1990-08-12|
|1002  |Jane Smith     |Product Manager      |90000 |1982-11-20|
|4025  |Jessica Brown  |HR Specialist        |60000 |1991-04-03|
|1001  |John Doe       |Software Engineer    |80000 |1985-05-15|
|1026  |Michael Johnson|Data Analyst         |65000 |1988-03-01|
|1313  |Olivia Nguyen  |Accounting Analyst   |75000 |1989-07-10|
|1212  |Robert Garcia  |Sales Representative |70000 |1986-11-28|
|1050  |Sarah Kim      |Marketing Coordinator|55000 |1987-06-25|
|1089  |Tom Wilson     |Project Manager      |85000 |1983-09-18|
+------+---------------+---------------------+------+----------+






----------------------------------------------------------------------------------------------------------------------------------------------------------------


